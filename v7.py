import os
import sys
import time
import json
import pandas as pd
import re
from datetime import datetime
from typing import List, Optional, Dict
from dataclasses import dataclass, asdict
import logging

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from bs4 import BeautifulSoup

# ================== ØªÙ†Ø¸ÛŒÙ… encoding ==================
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

# ================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ ==================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ifb_scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

IFB_MAIN_URL = "https://www.ifb.ir/Finstars/AllCrowdFundingProject.aspx"


@dataclass
class IFBProject:
    row_number: str
    project_name: str
    company_name: str
    national_id: str
    platform_url: str
    status: str
    fund_collection_start_date: str
    project_end_date: str
    description: str
    documents_url: str
    scraped_date: str
    # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (Ø§Ø² Ø³Ú©Ùˆ)
    target_amount: Optional[str] = None
    collected_amount: Optional[str] = None
    progress_percentage: Optional[str] = None
    expected_return: Optional[str] = None
    project_duration: Optional[str] = None
    capital_guarantee: Optional[str] = None
    project_type: Optional[str] = None
    project_symbol: Optional[str] = None
    investor_count: Optional[str] = None
    profit_payment_frequency: Optional[str] = None
    start_date_on_platform: Optional[str] = None
    platform_name: Optional[str] = None
    financial_institution: Optional[str] = None
    project_id_on_platform: Optional[str] = None
    thumbnail_url: Optional[str] = None
    applicant_name: Optional[str] = None

    def to_dict(self):
        return asdict(self)


class IFBScraper:
    def __init__(self, headless: bool = False):
        self.config = {
            'headless': headless,
            'timeout': 30,
            'implicit_wait': 10,
            'delay': 3,
        }
        self.driver = self._init_driver()
        self.wait = WebDriverWait(self.driver, self.config['timeout'])

    def _init_driver(self) -> webdriver.Chrome:
        script_dir = os.path.dirname(__file__)
        possible_paths = [
            os.path.join(script_dir, 'chromedriver.exe'),
            os.path.join(script_dir, 'chromedriver-win64', 'chromedriver.exe'),
            os.path.join(script_dir, 'chromedriver'),
            'chromedriver',
            'chromedriver.exe'
        ]
        chromedriver_path = None
        for path in possible_paths:
            if os.path.exists(path):
                chromedriver_path = path
                break
        if not chromedriver_path:
            raise FileNotFoundError("chromedriver.exe ÛŒØ§ÙØª Ù†Ø´Ø¯!")

        service = Service(chromedriver_path)
        options = Options()
        if self.config['headless']:
            options.add_argument("--headless=new")
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_argument('--disable-blink-features=AutomationControlled')

        driver = webdriver.Chrome(service=service, options=options)
        driver.implicitly_wait(self.config['implicit_wait'])
        logger.info(f"ChromeDriver Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯: {chromedriver_path}")
        return driver

    # ========== Ù…ØªØ¯Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¬ÛŒÙ†ÛŒØ´Ù† Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³ ==========

    def _navigate_to_page(self, page_number: int) -> bool:
        """Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ Ù…Ø´Ø®Øµ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² __doPostBack"""
        try:
            event_target = 'ctl00$ContentPlaceHolder1$grdCrowdFundingData'
            event_argument = f'Page${page_number}'
            script = f"__doPostBack('{event_target}', '{event_argument}');"
            self.driver.execute_script(script)
            time.sleep(self.config['delay'] * 2)
            WebDriverWait(self.driver, self.config['timeout']).until(
                EC.presence_of_element_located((By.ID, "ContentPlaceHolder1_grdCrowdFundingData"))
            )
            return True
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ø§ÙˆØ¨Ø±ÛŒ Ø¨Ù‡ ØµÙØ­Ù‡ {page_number}: {e}")
            return False

    def _extract_description_from_modal(self, desc_id: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø² modal (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† jQuery)"""
        try:
            self.driver.execute_script(f"showDesc('{desc_id}');")
            time.sleep(2)

            # Ø±ÙˆØ´ Û±: JavaScript
            desc_script = """
                var el = document.getElementById('Message');
                return el ? el.innerText || el.textContent : '';
            """
            description = self.driver.execute_script(desc_script)
            if description:
                # Ø¨Ø³ØªÙ† modal Ø¨Ø§ JS Ø®Ø§Ù„Øµ
                self.driver.execute_script("""
                    var modal = document.getElementById('FileForm');
                    if (modal) {
                        modal.style.display = 'none';
                        modal.classList.remove('in');
                    }
                    var backdrops = document.getElementsByClassName('modal-backdrop');
                    for(var i=0; i<backdrops.length; i++) backdrops[i].remove();
                    document.body.classList.remove('modal-open');
                """)
                return description.strip()

            # Ø±ÙˆØ´ Û²: BeautifulSoup
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            msg = soup.find('small', {'id': 'Message'}) or soup.find('div', {'id': 'Message'})
            if msg:
                description = msg.get_text(strip=True)
                # Ø¨Ø³ØªÙ† modal
                self.driver.execute_script("""
                    var modal = document.getElementById('FileForm');
                    if (modal) {
                        modal.style.display = 'none';
                        modal.classList.remove('in');
                    }
                    var backdrops = document.getElementsByClassName('modal-backdrop');
                    for(var i=0; i<backdrops.length; i++) backdrops[i].remove();
                    document.body.classList.remove('modal-open');
                """)
                return description

            return "ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª ID {desc_id}: {e}")
            return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªÙˆØ¶ÛŒØ­Ø§Øª"

    def _extract_current_page_projects(self) -> List[IFBProject]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙØ­Ù‡ Ø¬Ø§Ø±ÛŒ Ø§Ø² Ø¬Ø¯ÙˆÙ„ ÙØ±Ø§Ø¨ÙˆØ±Ø³"""
        projects = []
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        table = soup.find('table', {'id': 'ContentPlaceHolder1_grdCrowdFundingData'})
        if not table:
            logger.error("Ø¬Ø¯ÙˆÙ„ Ø¯Ø± ØµÙØ­Ù‡ Ø¬Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return projects

        rows = table.find_all('tr')
        for row in rows[1:]:
            cells = row.find_all('td')
            if len(cells) >= 10:
                try:
                    row_number = cells[0].text.strip()
                    project_name = cells[1].text.strip()
                    company_name = cells[2].text.strip()
                    national_id = cells[3].text.strip()

                    platform_link = cells[4].find('a')
                    platform_url = platform_link['href'] if platform_link else ""

                    status = cells[5].text.strip()
                    start_date = cells[6].text.strip()
                    end_date = cells[7].text.strip()

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª
                    description = "ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
                    details_link = cells[8].find('a')
                    if details_link and 'onclick' in details_link.attrs:
                        onclick = details_link['onclick']
                        match = re.search(r"showDesc\('(\d+)'\)", onclick)
                        if match:
                            desc_id = match.group(1)
                            description = self._extract_description_from_modal(desc_id)

                    # Ù„ÛŒÙ†Ú© Ù…Ø¯Ø§Ø±Ú©
                    documents_url = ""
                    documents_cell = cells[9]
                    documents_link = documents_cell.find('i', {'class': 'icon-folder'})
                    if documents_link and 'onclick' in documents_link.attrs:
                        onclick = documents_link['onclick']
                        match = re.search(r"GoToDocuments\('(\d+)'\)", onclick)
                        if match:
                            doc_id = match.group(1)
                            documents_url = f"{IFB_MAIN_URL}?doc_id={doc_id}"

                    project = IFBProject(
                        row_number=row_number,
                        project_name=project_name,
                        company_name=company_name,
                        national_id=national_id,
                        platform_url=platform_url,
                        status=status,
                        fund_collection_start_date=start_date,
                        project_end_date=end_date,
                        description=description,
                        documents_url=documents_url,
                        scraped_date=datetime.now().strftime('%Y/%m/%d %H:%M:%S')
                    )
                    projects.append(project)
                    logger.info(f"Ø±Ø¯ÛŒÙ {row_number}: {project_name} - ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ {start_date}")
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø±Ø¯ÛŒÙ: {e}")
        return projects

    def scrape_all_pages(self) -> List[IFBProject]:
        """Ù¾ÛŒÙ…Ø§ÛŒØ´ ØµÙØ­Ø§Øª ÙØ±Ø§Ø¨ÙˆØ±Ø³ ØªØ§ Ù…ÙˆØ§Ø¬Ù‡Ù‡ Ø¨Ø§ ØªØ§Ø±ÛŒØ® ØºÛŒØ± Û±Û´Û°Û´"""
        all_projects = []
        page_num = 1
        stop_pagination = False

        logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ù†Ø¯ ØµÙØ­Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³")
        self.driver.get(IFB_MAIN_URL)
        time.sleep(self.config['delay'])

        while not stop_pagination:
            logger.info(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ {page_num} ...")

            if page_num > 1:
                if not self._navigate_to_page(page_num):
                    logger.info("Ø§Ù…Ú©Ø§Ù† Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                    break

            page_projects = self._extract_current_page_projects()
            if not page_projects:
                logger.info(f"ØµÙØ­Ù‡ {page_num} Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ù†Ø¯Ø§Ø±Ø¯.")
                break

            page_has_non_1404 = False
            for proj in page_projects:
                start = proj.fund_collection_start_date
                year_match = re.search(r'(\d{4})', start)
                if year_match:
                    year = year_match.group(1)
                    if year != "1404":
                        logger.info(f"â— Ù¾Ø±ÙˆÚ˜Ù‡ {proj.project_name} Ø¯Ø§Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {start} (ØºÛŒØ± Û±Û´Û°Û´)")
                        page_has_non_1404 = True
                    else:
                        all_projects.append(proj)
                else:
                    logger.warning(f"ØªØ§Ø±ÛŒØ® Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {start}")

            logger.info(f"âœ… ØµÙØ­Ù‡ {page_num}: {len(page_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ØŒ {len(all_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Û±Û´Û°Û´ (Ù…Ø¬Ù…ÙˆØ¹)")

            if page_has_non_1404:
                logger.info(f"ğŸ›‘ ØªÙˆÙ‚Ù Ù¾ÛŒØ¬ÛŒÙ†ÛŒØ´Ù† Ø¯Ø± ØµÙØ­Ù‡ {page_num}")
                break

            page_num += 1

        logger.info(f"ğŸ¯ Ù¾Ø§ÛŒØ§Ù†: {len(all_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Û±Û´Û°Û´")
        return all_projects

    # ========== Ù…ØªØ¯Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø² Ø³Ú©ÙˆÙ‡Ø§ ==========

    def enrich_projects_with_platform_details(self, projects: List[IFBProject]) -> List[Dict]:
        enriched = []
        total = len(projects)
        for idx, project in enumerate(projects, 1):
            logger.info(f"\n[{idx}/{total}] Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±ÙˆÚ˜Ù‡: {project.project_name}")
            combined = project.to_dict()
            try:
                details = self._scrape_single_platform(project)
                combined.update(details)
                logger.info(f"   âœ… {len(details)} ÙÛŒÙ„Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
            except Exception as e:
                logger.error(f"   âŒ Ø®Ø·Ø§: {e}")
            enriched.append(combined)
            time.sleep(self.config['delay'])
        return enriched

    def _scrape_single_platform(self, project: IFBProject) -> Dict:
        url = project.platform_url
        if not url:
            return {}
        domain = url.lower()
        if 'hamafarin.ir' in domain:
            return self._scrape_hamafarin(project)
        elif 'fundocrowd.ir' in domain:
            return self._scrape_fundocrowd(project)
        elif 'karencrowd.com' in domain:
            return self._scrape_karencrowd(project)
        else:
            return self._scrape_generic(project)

    # ---------- Ù‡Ù…â€ŒØ¢ÙØ±ÛŒÙ† ----------
    def _scrape_hamafarin(self, project: IFBProject) -> Dict:
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            if "businessplans" not in self.driver.current_url:
                try:
                    view_all = self.driver.find_element(By.CSS_SELECTOR, "a[href='/businessplans']")
                    view_all.click()
                    time.sleep(self.config['delay'])
                except:
                    self.driver.get("https://hamafarin.ir/businessplans")
                    time.sleep(self.config['delay'])

            self._scroll_page()
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_=lambda c: c and 'w-full flex flex-col gap-y-4 group' in c)

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('a', class_=lambda c: c and 'text-[#2E2300]' in c)
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_hamafarin_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù‡Ù…â€ŒØ¢ÙØ±ÛŒÙ†: {e}")
        return details

    def _extract_hamafarin_card(self, card) -> Dict:
        d = {}
        title = card.find('a', class_=lambda c: c and 'text-[#2E2300]' in c)
        if title:
            d['title_on_platform'] = title.text.strip()
        link = card.find('a', href=re.compile(r'/businessplans/\d+'))
        if link and 'href' in link.attrs:
            match = re.search(r'/businessplans/(\d+)', link['href'])
            if match:
                d['project_id_on_platform'] = match.group(1)
        img = card.find('img')
        if img and img.get('src'):
            d['thumbnail_url'] = img['src']
        fin = card.find('p', string=re.compile('Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ:'))
        if fin:
            d['financial_institution'] = fin.text.strip()
        exec_p = card.find('p', class_='text-black17 font-YekanBakh text-md')
        if exec_p:
            d['applicant_name'] = exec_p.text.strip()
        bottom = card.find('div', class_=lambda c: c and 'bg-white' in c and '!pb-12' in c)
        if bottom:
            status_p = bottom.find('p', class_=lambda c: c and ('text-green67' in c or 'text-primary' in c))
            if status_p:
                d['status_on_platform'] = status_p.text.strip()
            perc = bottom.find('p', class_=lambda c: c and 'text-black17/70' in c)
            if perc and '%' in perc.text:
                d['progress_percentage'] = perc.text.strip()
            grid = bottom.find('div', class_=lambda c: c and 'grid-cols-3' in c)
            if grid:
                for item in grid.find_all('div', class_=lambda c: c and 'flex flex-col items-center gap-y-1' in c):
                    label = item.find('p', class_='text-gray-500')
                    value = item.find('p', class_=lambda c: c and 'text-gray-700' in c and 'font-bold' in c)
                    if label and value:
                        lbl = label.text.strip()
                        val = value.text.strip()
                        if 'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù' in lbl:
                            d['target_amount'] = val
                        elif 'Ù¾ÛŒØ´Ø¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯' in lbl:
                            d['expected_return'] = val
                        elif 'Ù…Ø¯Øª Ø·Ø±Ø­' in lbl:
                            d['project_duration'] = val
                        elif 'ØªØ¶Ù…ÛŒÙ† Ø§ØµÙ„ Ø³Ø±Ù…Ø§ÛŒÙ‡' in lbl:
                            d['capital_guarantee'] = val
                        elif 'Ù†ÙˆØ¹ Ø·Ø±Ø­' in lbl:
                            d['project_type'] = val
                        elif 'Ù†Ù…Ø§Ø¯ Ø·Ø±Ø­' in lbl:
                            d['project_symbol'] = val
                        elif 'ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹' in lbl:
                            d['start_date_on_platform'] = val
                        elif 'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±Ø§Ù†' in lbl:
                            d['investor_count'] = re.sub(r'[^\d]', '', val)
                        elif 'ØªÙˆØ§ØªØ± Ù¾Ø±Ø¯Ø§Ø®Øª Ø³ÙˆØ¯' in lbl:
                            d['profit_payment_frequency'] = val
        return d

    # ---------- ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯ ----------
    def _scrape_fundocrowd(self, project: IFBProject) -> Dict:
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'] * 2)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_='home-box-design')

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('h5', class_='main-h2')
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_fundocrowd_card(card)
                    if not details.get('expected_return') or not details.get('project_duration'):
                        details.update(self._scrape_fundocrowd_details(card))
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return details

    def _extract_fundocrowd_card(self, card) -> Dict:
        d = {}
        try:
            title = card.find('h5', class_='main-h2')
            if title:
                d['title_on_platform'] = title.text.strip()
            img = card.find('img', src=re.compile(r'common/DownloadFile'))
            if img and img.get('src'):
                d['thumbnail_url'] = img['src']
            company_span = card.find('span', string=re.compile(r'Ø´Ø±Ú©Øª'))
            if company_span:
                d['applicant_name'] = company_span.parent.get_text(strip=True) if company_span.parent else company_span.text
            target_div = card.find('div', class_='d-flex mt-3')
            if target_div:
                spans = target_div.find_all('span')
                if len(spans) >= 2:
                    d['target_amount'] = spans[0].text.strip()
                    d['progress_percentage'] = spans[1].text.strip()
            progress_bar = card.find('div', class_='progress-bar')
            if progress_bar and progress_bar.has_attr('style'):
                match = re.search(r'width:\s*(\d+)%', progress_bar['style'])
                if match:
                    d['progress_width'] = match.group(1)
            duration_div = card.find('div', class_='row mt-3 ml-0')
            if duration_div:
                cols = duration_div.find_all('div', class_='col')
                if len(cols) >= 2:
                    duration_b = cols[0].find('b')
                    if duration_b:
                        d['project_duration'] = duration_b.text.strip()
                    profit_b = cols[1].find('b')
                    if profit_b:
                        d['expected_return'] = profit_b.text.strip()
            detail_link = card.find('a', href=re.compile(r'/companyDetail/\d+'))
            if detail_link and 'href' in detail_link.attrs:
                d['details_page_url'] = "https://fundocrowd.ir" + detail_link['href']
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ø±Øª ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return d

    def _scrape_fundocrowd_details(self, card) -> Dict:
        d = {}
        try:
            detail_link = card.find('a', href=re.compile(r'/companyDetail/\d+'))
            if not detail_link or 'href' not in detail_link.attrs:
                return d
            href = detail_link['href']
            full_url = "https://fundocrowd.ir" + href
            self.driver.get(full_url)
            time.sleep(self.config['delay'])

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            payment_div = soup.find('div', class_='detail-little-b')
            if payment_div:
                p = payment_div.find('p', class_='main-h2')
                if p:
                    d['profit_payment_frequency'] = p.text.strip()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return d

    # ---------- Ú©Ø§Ø±Ù†â€ŒÚ©Ø±Ø§Ø¯ ----------
    def _scrape_karencrowd(self, project: IFBProject) -> Dict:
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            if "plans" not in self.driver.current_url:
                try:
                    view_all = self.driver.find_element(By.XPATH, "//a[contains(text(), 'Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù‡Ù…Ù‡ Ø·Ø±Ø­â€ŒÙ‡Ø§')]")
                    view_all.click()
                    time.sleep(self.config['delay'])
                except:
                    self.driver.get("https://www.karencrowd.com/plans")
                    time.sleep(self.config['delay'])

            self._scroll_page()
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_=lambda c: c and 'flex flex-col' in c and 'h-[775px]' in c)
            if not cards:
                cards = soup.find_all('div', class_=lambda c: c and 'bg-white' in c and 'shadow-md' in c)

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('h2', class_='text-xl font-bold')
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_karencrowd_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú©Ø§Ø±Ù†â€ŒÚ©Ø±Ø§Ø¯: {e}")
        return details

    def _extract_karencrowd_card(self, card) -> Dict:
        d = {}
        title = card.find('h2', class_='text-xl font-bold')
        if title:
            d['title_on_platform'] = title.text.strip()
        link = card.find('a', href=re.compile(r'/plans/\d+'))
        if link and 'href' in link.attrs:
            match = re.search(r'/plans/(\d+)', link['href'])
            if match:
                d['project_id_on_platform'] = match.group(1)
        img = card.find('img')
        if img and img.get('src'):
            d['thumbnail_url'] = img['src']
        target_label = card.find('span', string=re.compile('Ù…Ø¨Ù„Øº Ù‡Ø¯Ù'))
        if target_label:
            parent = target_label.find_parent('div', class_='grid')
            if parent:
                cols = parent.find_all('div', class_='text-xs text-center')
                for col in cols:
                    label_span = col.find('span', class_='text-gray-card')
                    value_span = col.find('span', class_='text-dark font-bold')
                    if label_span and value_span:
                        lbl = label_span.text.strip()
                        val = value_span.text.strip()
                        if 'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù' in lbl:
                            d['target_amount'] = val
                        elif 'Ù…Ø¯Øª Ø·Ø±Ø­' in lbl:
                            d['project_duration'] = val
                        elif 'Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯' in lbl:
                            d['expected_return'] = val
        return d

    # ---------- Ù…ØªØ¯ Ø¹Ù…ÙˆÙ…ÛŒ ----------
    def _scrape_generic(self, project: IFBProject) -> Dict:
        d = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            page_text = soup.get_text()
            patterns = {
                'target_amount': [r'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù.*?([\d,Ù¬]+)', r'Ù‡Ø¯Ù.*?([\d,Ù¬]+)\s*ØªÙˆÙ…Ø§Ù†', r'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø².*?([\d,Ù¬]+)'],
                'expected_return': [r'(\d+\.?\d*)\s*Ùª', r'Ø³ÙˆØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ.*?(\d+\.?\d*)', r'Ø¨Ø§Ø²Ø¯Ù‡.*?(\d+\.?\d*)'],
                'project_duration': [r'(\d+)\s*Ù…Ø§Ù‡', r'Ù…Ø¯Øª Ø·Ø±Ø­.*?(\d+)\s*Ù…Ø§Ù‡'],
                'investor_count': [r'(\d+)\s*Ù†ÙØ±', r'ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±.*?(\d+)'],
            }
            for field, pat_list in patterns.items():
                for pat in pat_list:
                    match = re.search(pat, page_text, re.IGNORECASE)
                    if match:
                        d[field] = match.group(1)
                        break
            company_patterns = [r'Ø´Ø±Ú©Øª\s*([\w\s]+)', r'Ù…ØªÙ‚Ø§Ø¶ÛŒ\s*:\s*([\w\s]+)']
            for pat in company_patterns:
                match = re.search(pat, page_text)
                if match:
                    d['applicant_name'] = match.group(1).strip()
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…ØªØ¯ Ø¹Ù…ÙˆÙ…ÛŒ: {e}")
        return d

    def _scroll_page(self, times=2):
        for _ in range(times):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

    # ========== Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ==========
    def save_combined_data(self, data: List[Dict], base_name: str = "ifb_projects_1404_with_details"):
        json_file = f"{base_name}.json"
        csv_file = f"{base_name}.csv"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info(f"ÙØ§ÛŒÙ„ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {json_file}")

        df = pd.DataFrame(data)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logger.info(f"ÙØ§ÛŒÙ„ CSV Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {csv_file}")

    def close(self):
        if self.driver:
            self.driver.quit()
            logger.info("Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø³ØªÙ‡ Ø´Ø¯")


def main():
    logger.info("=" * 60)
    logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø·Ø±Ø­â€ŒÙ‡Ø§ÛŒ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ Ø¬Ù…Ø¹ÛŒ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†")
    logger.info(f"Ø¢Ø¯Ø±Ø³: {IFB_MAIN_URL}")
    logger.info("=" * 60)

    scraper = IFBScraper(headless=False)
    try:
        projects = scraper.scrape_all_pages()
        if not projects:
            logger.warning("Ù‡ÛŒÚ† Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø§ ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Û±Û´Û°Û´ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return

        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ {len(projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Û±Û´Û°Û´ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")

        logger.info("\n" + "=" * 60)
        logger.info("Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø² Ø³Ú©ÙˆÙ‡Ø§")
        logger.info("=" * 60)
        enriched = scraper.enrich_projects_with_platform_details(projects)

        scraper.save_combined_data(enriched, "ifb_projects_1404_complete")

        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡
        logger.info("\nğŸ“Š Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ:")
        for i, item in enumerate(enriched[:3]):
            logger.info(f"\nÙ¾Ø±ÙˆÚ˜Ù‡ {i+1}: {item.get('project_name')}")
            logger.info(f"   Ù…Ø¨Ù„Øº Ù‡Ø¯Ù: {item.get('target_amount', '---')}")
            logger.info(f"   Ù…Ø¯Øª Ø·Ø±Ø­: {item.get('project_duration', '---')}")
            logger.info(f"   Ø³ÙˆØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: {item.get('expected_return', '---')}")
            logger.info(f"   Ù…ØªÙ‚Ø§Ø¶ÛŒ: {item.get('applicant_name', '---')}")
            logger.info(f"   ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Ø¯Ø± Ø³Ú©Ùˆ: {item.get('start_date_on_platform', '---')}")

    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        scraper.close()


if __name__ == "__main__":
    main()