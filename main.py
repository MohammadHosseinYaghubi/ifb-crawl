#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import re
import time
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
import logging
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException

import gspread
from google.oauth2.service_account import Credentials
from bs4 import BeautifulSoup

# -------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ --------------------
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ifb_scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

IFB_MAIN_URL = "https://www.ifb.ir/Finstars/AllCrowdFundingProject.aspx"
SHEET_NAME = "Crowdfunding_Projects_1404"
CREDS_ENV_VAR = "GOOGLE_CREDENTIALS"

# -------------------- Ù…Ø¯Ù„ Ø¯Ø§Ø¯Ù‡ --------------------
@dataclass
class IFBProject:
    row_number: str
    project_name: str
    company_name: str
    national_id: str
    platform_url: str
    status: str
    fund_collection_start_date: str
    project_end_date: str
    description: str
    documents_url: str
    scraped_date: str
    ifb_project_id: Optional[str] = None          # Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§ÛŒ ÙØ±Ø§Ø¨ÙˆØ±Ø³ (Ø§Ø² showDesc)
    # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ø§Ø² Ø³Ú©Ùˆ
    target_amount: Optional[str] = None
    collected_amount: Optional[str] = None
    progress_percentage: Optional[str] = None
    expected_return: Optional[str] = None
    project_duration: Optional[str] = None
    capital_guarantee: Optional[str] = None
    project_type: Optional[str] = None
    project_symbol: Optional[str] = None
    investor_count: Optional[str] = None
    profit_payment_frequency: Optional[str] = None
    start_date_on_platform: Optional[str] = None
    platform_name: Optional[str] = None
    financial_institution: Optional[str] = None
    project_id_on_platform: Optional[str] = None
    thumbnail_url: Optional[str] = None
    applicant_name: Optional[str] = None

    def to_dict(self):
        return asdict(self)


# -------------------- Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø§Ø³Ú©Ø±Ù¾Ø± ÙØ±Ø§Ø¨ÙˆØ±Ø³ --------------------
class IFBScraper:
    def __init__(self, headless: bool = False):
        self.config = {
            'headless': headless,
            'timeout': 30,
            'implicit_wait': 10,
            'delay': 3,
        }
        self.driver = self._init_driver()
        self.wait = WebDriverWait(self.driver, self.config['timeout'])

    def _init_driver(self) -> webdriver.Chrome:
        script_dir = os.path.dirname(__file__)
        possible_paths = [
            os.path.join(script_dir, 'chromedriver.exe'),
            os.path.join(script_dir, 'chromedriver-win64', 'chromedriver.exe'),
            os.path.join(script_dir, 'chromedriver'),
            'chromedriver',
            'chromedriver.exe'
        ]
        chromedriver_path = None
        for path in possible_paths:
            if os.path.exists(path):
                chromedriver_path = path
                break
        if not chromedriver_path:
            raise FileNotFoundError("chromedriver.exe ÛŒØ§ÙØª Ù†Ø´Ø¯!")

        service = Service(chromedriver_path)
        options = Options()
        if self.config['headless']:
            options.add_argument("--headless=new")
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_argument('--disable-blink-features=AutomationControlled')

        driver = webdriver.Chrome(service=service, options=options)
        driver.implicitly_wait(self.config['implicit_wait'])
        logger.info(f"ChromeDriver Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯: {chromedriver_path}")
        return driver

    # ---------- Ù…ØªØ¯Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¬ÛŒÙ†ÛŒØ´Ù† Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³ ----------
    def _navigate_to_page(self, page_number: int) -> bool:
        try:
            event_target = 'ctl00$ContentPlaceHolder1$grdCrowdFundingData'
            event_argument = f'Page${page_number}'
            script = f"__doPostBack('{event_target}', '{event_argument}');"
            self.driver.execute_script(script)
            time.sleep(self.config['delay'] * 2)
            WebDriverWait(self.driver, self.config['timeout']).until(
                EC.presence_of_element_located((By.ID, "ContentPlaceHolder1_grdCrowdFundingData"))
            )
            return True
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ø§ÙˆØ¨Ø±ÛŒ Ø¨Ù‡ ØµÙØ­Ù‡ {page_number}: {e}")
            return False

    def _extract_description_from_modal(self, desc_id: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø² modal (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† jQuery)"""
        try:
            self.driver.execute_script(f"showDesc('{desc_id}');")
            time.sleep(2)

            desc_script = """
                var el = document.getElementById('Message');
                return el ? el.innerText || el.textContent : '';
            """
            description = self.driver.execute_script(desc_script)
            if description:
                self.driver.execute_script("""
                    var modal = document.getElementById('FileForm');
                    if (modal) {
                        modal.style.display = 'none';
                        modal.classList.remove('in');
                    }
                    var backdrops = document.getElementsByClassName('modal-backdrop');
                    for(var i=0; i<backdrops.length; i++) backdrops[i].remove();
                    document.body.classList.remove('modal-open');
                """)
                return description.strip()

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            msg = soup.find('small', {'id': 'Message'}) or soup.find('div', {'id': 'Message'})
            if msg:
                description = msg.get_text(strip=True)
                self.driver.execute_script("""
                    var modal = document.getElementById('FileForm');
                    if (modal) {
                        modal.style.display = 'none';
                        modal.classList.remove('in');
                    }
                    var backdrops = document.getElementsByClassName('modal-backdrop');
                    for(var i=0; i<backdrops.length; i++) backdrops[i].remove();
                    document.body.classList.remove('modal-open');
                """)
                return description

            return "ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª ID {desc_id}: {e}")
            return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªÙˆØ¶ÛŒØ­Ø§Øª"

    def _extract_current_page_projects(self) -> List[IFBProject]:
        projects = []
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        table = soup.find('table', {'id': 'ContentPlaceHolder1_grdCrowdFundingData'})
        if not table:
            logger.error("Ø¬Ø¯ÙˆÙ„ Ø¯Ø± ØµÙØ­Ù‡ Ø¬Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return projects

        rows = table.find_all('tr')
        for row in rows[1:]:
            cells = row.find_all('td')
            if len(cells) >= 10:
                try:
                    row_number = cells[0].text.strip()
                    project_name = cells[1].text.strip()
                    company_name = cells[2].text.strip()
                    national_id = cells[3].text.strip()
                    platform_link = cells[4].find('a')
                    platform_url = platform_link['href'] if platform_link else ""
                    status = cells[5].text.strip()
                    start_date = cells[6].text.strip()
                    end_date = cells[7].text.strip()

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ùˆ Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§
                    ifb_id = None
                    description = "ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
                    details_link = cells[8].find('a')
                    if details_link and 'onclick' in details_link.attrs:
                        onclick = details_link['onclick']
                        match = re.search(r"showDesc\('(\d+)'\)", onclick)
                        if match:
                            ifb_id = match.group(1)
                            description = self._extract_description_from_modal(ifb_id)

                    # Ø§Ú¯Ø± Ø§Ø² Ù„ÛŒÙ†Ú© ØªÙˆØ¶ÛŒØ­Ø§Øª id Ú¯Ø±ÙØªÙ‡ Ù†Ø´Ø¯ØŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ù…Ø¯Ø§Ø±Ú© Ø¨Ú¯ÛŒØ±
                    if not ifb_id:
                        documents_cell = cells[9]
                        documents_link = documents_cell.find('i', {'class': 'icon-folder'})
                        if documents_link and 'onclick' in documents_link.attrs:
                            onclick = documents_link['onclick']
                            match = re.search(r"GoToDocuments\('(\d+)'\)", onclick)
                            if match:
                                ifb_id = match.group(1)

                    documents_url = ""
                    if ifb_id:  # Ø³Ø§Ø®Øª documents_url Ø¨Ø§ Ù‡Ù…Ø§Ù† id
                        documents_url = f"{IFB_MAIN_URL}?doc_id={ifb_id}"

                    project = IFBProject(
                        row_number=row_number,
                        project_name=project_name,
                        company_name=company_name,
                        national_id=national_id,
                        platform_url=platform_url,
                        status=status,
                        fund_collection_start_date=start_date,
                        project_end_date=end_date,
                        description=description,
                        documents_url=documents_url,
                        scraped_date=datetime.now().strftime('%Y/%m/%d %H:%M:%S'),
                        ifb_project_id=ifb_id
                    )
                    projects.append(project)
                    logger.info(f"Ø±Ø¯ÛŒÙ {row_number}: {project_name} - Ø´Ù†Ø§Ø³Ù‡ {ifb_id}")
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø±Ø¯ÛŒÙ: {e}")
        return projects

    def scrape_all_pages(self) -> List[IFBProject]:
        all_projects = []
        page_num = 1
        stop_pagination = False

        logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ù†Ø¯ ØµÙØ­Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³")
        self.driver.get(IFB_MAIN_URL)
        time.sleep(self.config['delay'])

        while not stop_pagination:
            logger.info(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ {page_num} ...")

            if page_num > 1:
                if not self._navigate_to_page(page_num):
                    logger.info("Ø§Ù…Ú©Ø§Ù† Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                    break

            page_projects = self._extract_current_page_projects()
            if not page_projects:
                logger.info(f"ØµÙØ­Ù‡ {page_num} Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ù†Ø¯Ø§Ø±Ø¯.")
                break

            page_has_non_1404 = False
            for proj in page_projects:
                start = proj.fund_collection_start_date
                year_match = re.search(r'(\d{4})', start)
                if year_match:
                    year = year_match.group(1)
                    if year != "1404":
                        logger.info(f"â— Ù¾Ø±ÙˆÚ˜Ù‡ {proj.project_name} Ø¯Ø§Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {start} (ØºÛŒØ± Û±Û´Û°Û´)")
                        page_has_non_1404 = True
                    else:
                        all_projects.append(proj)
                else:
                    logger.warning(f"ØªØ§Ø±ÛŒØ® Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {start}")

            logger.info(f"âœ… ØµÙØ­Ù‡ {page_num}: {len(page_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ØŒ {len(all_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Û±Û´Û°Û´ (Ù…Ø¬Ù…ÙˆØ¹)")

            if page_has_non_1404:
                logger.info(f"ğŸ›‘ ØªÙˆÙ‚Ù Ù¾ÛŒØ¬ÛŒÙ†ÛŒØ´Ù† Ø¯Ø± ØµÙØ­Ù‡ {page_num}")
                break

            page_num += 1

        logger.info(f"ğŸ¯ Ù¾Ø§ÛŒØ§Ù†: {len(all_projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Û±Û´Û°Û´")
        return all_projects

    def close(self):
        if self.driver:
            self.driver.quit()
            logger.info("Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø³ØªÙ‡ Ø´Ø¯")


# -------------------- Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø³Ú©ÙˆÙ‡Ø§ --------------------
class PlatformDetailScraper:
    """
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ IFB Ø¨Ù‡ Ø¢Ø¯Ø±Ø³ Ø³Ú©Ùˆ Ø±ÙØªÙ‡ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ø±Øª Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯.
    ØªØ´Ø®ÛŒØµ Ø¯Ø§Ù…Ù†Ù‡ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø± Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    def __init__(self, driver: webdriver.Chrome, config: dict):
        self.driver = driver
        self.config = config
        self.wait = WebDriverWait(self.driver, config['timeout'])

    def scrape(self, project: IFBProject) -> Dict[str, Any]:
        if not project.platform_url:
            return {}

        domain = urlparse(project.platform_url).netloc.lower()
        logger.info(f"ğŸ” Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² {domain} Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ {project.project_name}")

        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…ØªØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù…Ù†Ù‡
        if 'hamafarin.ir' in domain:
            return self._scrape_hamafarin(project)
        elif 'fundocrowd.ir' in domain:
            return self._scrape_fundocrowd(project)
        elif 'karencrowd.com' in domain:
            return self._scrape_karencrowd(project)
        elif 'ifund.ir' in domain:
            return self._scrape_ifund(project)
        elif 'zeema.fund' in domain:
            return self._scrape_zeema(project)
        else:
            # Ù…ØªØ¯ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ø³Ú©ÙˆÙ‡Ø§
            return self._scrape_generic(project)

    # ---------- Ù…ØªØ¯Ù‡Ø§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ú©Ùˆ ----------
    def _scrape_hamafarin(self, project: IFBProject) -> Dict:
        """Ù‡Ù…â€ŒØ¢ÙØ±ÛŒÙ† â€“ Ø³Ø§Ø®ØªØ§Ø± Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ÛŒ"""
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])

            # Ø§Ú¯Ø± Ø¨Ù‡ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø±ÙØªØŒ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø·Ø±Ø­â€ŒÙ‡Ø§ Ø¨Ø±Ùˆ
            if "businessplans" not in self.driver.current_url:
                try:
                    view_all = self.driver.find_element(By.CSS_SELECTOR, "a[href='/businessplans']")
                    view_all.click()
                    time.sleep(self.config['delay'])
                except:
                    self.driver.get("https://hamafarin.ir/businessplans")
                    time.sleep(self.config['delay'])

            self._scroll_page()
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_=lambda c: c and 'w-full flex flex-col gap-y-4 group' in c)

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('a', class_=lambda c: c and 'text-[#2E2300]' in c)
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_hamafarin_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù‡Ù…â€ŒØ¢ÙØ±ÛŒÙ†: {e}")
        return details

    def _extract_hamafarin_card(self, card) -> Dict:
        d = {}
        # Ø¹Ù†ÙˆØ§Ù†
        title = card.find('a', class_=lambda c: c and 'text-[#2E2300]' in c)
        if title:
            d['title_on_platform'] = title.text.strip()
        # Ù„ÛŒÙ†Ú© Ùˆ Ø´Ù†Ø§Ø³Ù‡
        link = card.find('a', href=re.compile(r'/businessplans/\d+'))
        if link and 'href' in link.attrs:
            match = re.search(r'/businessplans/(\d+)', link['href'])
            if match:
                d['project_id_on_platform'] = match.group(1)
        # ØªØµÙˆÛŒØ±
        img = card.find('img')
        if img and img.get('src'):
            d['thumbnail_url'] = img['src']
        # Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ
        fin = card.find('p', string=re.compile('Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ:'))
        if fin:
            d['financial_institution'] = fin.text.strip()
        # Ù…Ø¬Ø±ÛŒ
        exec_p = card.find('p', class_='text-black17 font-YekanBakh text-md')
        if exec_p:
            d['applicant_name'] = exec_p.text.strip()
        # Ø¨Ø®Ø´ Ù¾Ø§ÛŒÛŒÙ†ÛŒ
        bottom = card.find('div', class_=lambda c: c and 'bg-white' in c and '!pb-12' in c)
        if bottom:
            # ÙˆØ¶Ø¹ÛŒØª
            status_p = bottom.find('p', class_=lambda c: c and ('text-green67' in c or 'text-primary' in c))
            if status_p:
                d['status_on_platform'] = status_p.text.strip()
            # Ø¯Ø±ØµØ¯
            perc = bottom.find('p', class_=lambda c: c and 'text-black17/70' in c)
            if perc and '%' in perc.text:
                d['progress_percentage'] = perc.text.strip()
            # Ú¯Ø±ÛŒØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            grid = bottom.find('div', class_=lambda c: c and 'grid-cols-3' in c)
            if grid:
                for item in grid.find_all('div', class_=lambda c: c and 'flex flex-col items-center gap-y-1' in c):
                    label = item.find('p', class_='text-gray-500')
                    value = item.find('p', class_=lambda c: c and 'text-gray-700' in c and 'font-bold' in c)
                    if label and value:
                        lbl = label.text.strip()
                        val = value.text.strip()
                        if 'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù' in lbl:
                            d['target_amount'] = val
                        elif 'Ù¾ÛŒØ´Ø¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯' in lbl:
                            d['expected_return'] = val
                        elif 'Ù…Ø¯Øª Ø·Ø±Ø­' in lbl:
                            d['project_duration'] = val
                        elif 'ØªØ¶Ù…ÛŒÙ† Ø§ØµÙ„ Ø³Ø±Ù…Ø§ÛŒÙ‡' in lbl:
                            d['capital_guarantee'] = val
                        elif 'Ù†ÙˆØ¹ Ø·Ø±Ø­' in lbl:
                            d['project_type'] = val
                        elif 'Ù†Ù…Ø§Ø¯ Ø·Ø±Ø­' in lbl:
                            d['project_symbol'] = val
                        elif 'ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹' in lbl:
                            d['start_date_on_platform'] = val
                        elif 'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±Ø§Ù†' in lbl:
                            d['investor_count'] = re.sub(r'[^\d]', '', val)
                        elif 'ØªÙˆØ§ØªØ± Ù¾Ø±Ø¯Ø§Ø®Øª Ø³ÙˆØ¯' in lbl:
                            d['profit_payment_frequency'] = val
        return d

    def _scrape_fundocrowd(self, project: IFBProject) -> Dict:
        """ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯ â€“ Ø³Ø§Ø®ØªØ§Ø± home-box-design"""
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'] * 2)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_='home-box-design')

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('h5', class_='main-h2')
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_fundocrowd_card(card)
                    # Ø§Ú¯Ø± Ø¨Ø±Ø®ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯ØŒ Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ù„ÛŒÚ© Ú©Ù†
                    if not details.get('expected_return') or not details.get('project_duration'):
                        details.update(self._scrape_fundocrowd_details(card))
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return details

    def _extract_fundocrowd_card(self, card) -> Dict:
        d = {}
        try:
            title = card.find('h5', class_='main-h2')
            if title:
                d['title_on_platform'] = title.text.strip()
            img = card.find('img', src=re.compile(r'common/DownloadFile'))
            if img and img.get('src'):
                d['thumbnail_url'] = img['src']
            company_span = card.find('span', string=re.compile(r'Ø´Ø±Ú©Øª'))
            if company_span:
                d['applicant_name'] = company_span.parent.get_text(strip=True) if company_span.parent else company_span.text
            target_div = card.find('div', class_='d-flex mt-3')
            if target_div:
                spans = target_div.find_all('span')
                if len(spans) >= 2:
                    d['target_amount'] = spans[0].text.strip()
                    d['progress_percentage'] = spans[1].text.strip()
            progress_bar = card.find('div', class_='progress-bar')
            if progress_bar and progress_bar.has_attr('style'):
                match = re.search(r'width:\s*(\d+)%', progress_bar['style'])
                if match:
                    d['progress_width'] = match.group(1)
            duration_div = card.find('div', class_='row mt-3 ml-0')
            if duration_div:
                cols = duration_div.find_all('div', class_='col')
                if len(cols) >= 2:
                    duration_b = cols[0].find('b')
                    if duration_b:
                        d['project_duration'] = duration_b.text.strip()
                    profit_b = cols[1].find('b')
                    if profit_b:
                        d['expected_return'] = profit_b.text.strip()
            detail_link = card.find('a', href=re.compile(r'/companyDetail/\d+'))
            if detail_link and 'href' in detail_link.attrs:
                d['details_page_url'] = "https://fundocrowd.ir" + detail_link['href']
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ø±Øª ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return d

    def _scrape_fundocrowd_details(self, card) -> Dict:
        d = {}
        try:
            detail_link = card.find('a', href=re.compile(r'/companyDetail/\d+'))
            if not detail_link or 'href' not in detail_link.attrs:
                return d
            href = detail_link['href']
            full_url = "https://fundocrowd.ir" + href
            self.driver.get(full_url)
            time.sleep(self.config['delay'])

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            payment_div = soup.find('div', class_='detail-little-b')
            if payment_div:
                p = payment_div.find('p', class_='main-h2')
                if p:
                    d['profit_payment_frequency'] = p.text.strip()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª ÙØ§Ù†Ø¯ÙˆÚ©Ø±Ø§Ø¯: {e}")
        return d

    def _scrape_karencrowd(self, project: IFBProject) -> Dict:
        """Ú©Ø§Ø±Ù†â€ŒÚ©Ø±Ø§Ø¯ â€“ Ù…Ø´Ø§Ø¨Ù‡ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ"""
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])

            if "plans" not in self.driver.current_url:
                try:
                    view_all = self.driver.find_element(By.XPATH, "//a[contains(text(), 'Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù‡Ù…Ù‡ Ø·Ø±Ø­â€ŒÙ‡Ø§')]")
                    view_all.click()
                    time.sleep(self.config['delay'])
                except:
                    self.driver.get("https://www.karencrowd.com/plans")
                    time.sleep(self.config['delay'])

            self._scroll_page()
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_=lambda c: c and 'flex flex-col' in c and 'h-[775px]' in c)
            if not cards:
                cards = soup.find_all('div', class_=lambda c: c and 'bg-white' in c and 'shadow-md' in c)

            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('h2', class_='text-xl font-bold')
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_karencrowd_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú©Ø§Ø±Ù†â€ŒÚ©Ø±Ø§Ø¯: {e}")
        return details

    def _extract_karencrowd_card(self, card) -> Dict:
        d = {}
        title = card.find('h2', class_='text-xl font-bold')
        if title:
            d['title_on_platform'] = title.text.strip()
        link = card.find('a', href=re.compile(r'/plans/\d+'))
        if link and 'href' in link.attrs:
            match = re.search(r'/plans/(\d+)', link['href'])
            if match:
                d['project_id_on_platform'] = match.group(1)
        img = card.find('img')
        if img and img.get('src'):
            d['thumbnail_url'] = img['src']
        target_label = card.find('span', string=re.compile('Ù…Ø¨Ù„Øº Ù‡Ø¯Ù'))
        if target_label:
            parent = target_label.find_parent('div', class_='grid')
            if parent:
                cols = parent.find_all('div', class_='text-xs text-center')
                for col in cols:
                    label_span = col.find('span', class_='text-gray-card')
                    value_span = col.find('span', class_='text-dark font-bold')
                    if label_span and value_span:
                        lbl = label_span.text.strip()
                        val = value_span.text.strip()
                        if 'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù' in lbl:
                            d['target_amount'] = val
                        elif 'Ù…Ø¯Øª Ø·Ø±Ø­' in lbl:
                            d['project_duration'] = val
                        elif 'Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯' in lbl:
                            d['expected_return'] = val
        return d

    def _scrape_ifund(self, project: IFBProject) -> Dict:
        """Ø¢ÛŒâ€ŒÙØ§Ù†Ø¯ â€“ Ø³Ø§Ø®ØªØ§Ø± Ø¢ÛŒâ€ŒÙØ§Ù†Ø¯"""
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„
            cards = soup.find_all('div', class_=lambda c: c and 'col-span-1' in c and 'bg-white' in c)
            target = project.project_name.strip()
            for card in cards:
                title_elem = card.find('p', class_='text-lg lg:text-xl font-medium')
                if not title_elem:
                    continue
                card_title = title_elem.text.strip()
                if target in card_title or card_title in target:
                    details = self._extract_ifund_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¢ÛŒâ€ŒÙØ§Ù†Ø¯: {e}")
        return details

    def _extract_ifund_card(self, card) -> Dict:
        d = {}
        try:
            # Ø¹Ù†ÙˆØ§Ù†
            title = card.find('p', class_='text-lg lg:text-xl font-medium')
            if title:
                d['title_on_platform'] = title.text.strip()
            # Ø³ÙˆØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
            profit_span = card.find('span', class_='bg-custom-orange')
            if profit_span:
                d['expected_return'] = profit_span.text.strip()
            # Ù†Ù…Ø§Ø¯
            symbol_a = card.find('a', string=re.compile(r'ÙØ§Ù†Ø¯ÙˆÛŒØ±Ø§'))
            if symbol_a:
                d['project_symbol'] = symbol_a.text.strip()
            # Ù…Ø¨Ù„Øº Ù‡Ø¯Ù Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡
            divs = card.find_all('div', class_='flex justify-between text-base font-medium')
            if len(divs) >= 1:
                spans = divs[0].find_all('span')
                if len(spans) >= 2:
                    d['collected_amount'] = spans[0].text.strip()
                    d['target_amount'] = spans[1].text.strip()
            # Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒØŒ Ù…ØªÙ‚Ø§Ø¶ÛŒØŒ Ù…Ø¯ØªØŒ Ù†ÙˆØ¹ØŒ ØªØ¶Ù…ÛŒÙ† Ø§Ø² Ù„ÛŒØ³Øª
            items = card.find_all('div', class_='flex items-center justify-start text-black')
            for it in items:
                text = it.get_text(" ", strip=True)
                if 'Ø³Ú©ÙˆÛŒ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ Ø¬Ù…Ø¹ÛŒ Ø¢ÛŒÙØ§Ù†Ø¯' in text:
                    d['platform_name'] = 'Ø¢ÛŒâ€ŒÙØ§Ù†Ø¯'
                elif 'Ù†Ø§Ù… Ù…ØªÙ‚Ø§Ø¶ÛŒ :' in text:
                    d['applicant_name'] = text.replace('Ù†Ø§Ù… Ù…ØªÙ‚Ø§Ø¶ÛŒ :', '').strip()
                elif 'Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ :' in text:
                    d['financial_institution'] = text.replace('Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ :', '').strip()
                elif 'Ù…Ø¯Øª Ø·Ø±Ø­ :' in text:
                    d['project_duration'] = text.replace('Ù…Ø¯Øª Ø·Ø±Ø­ :', '').strip()
                elif 'Ù†Ù…Ø§Ø¯ Ø·Ø±Ø­ :' in text:
                    d['project_symbol'] = text.replace('Ù†Ù…Ø§Ø¯ Ø·Ø±Ø­ :', '').strip()
                elif 'Ù†ÙˆØ¹ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ :' in text:
                    d['project_type'] = text.replace('Ù†ÙˆØ¹ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ :', '').strip()
                elif 'Ø³ÙˆØ¯ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø³Ø§Ù„Ø§Ù†Ù‡:' in text:
                    # Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø² profit_span Ú¯Ø±ÙØªÛŒÙ…
                    pass
                elif 'Ù…ÙˆØ§Ø¹Ø¯ Ù¾Ø±Ø¯Ø§Ø®Øª Ø³ÙˆØ¯ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ :' in text:
                    d['profit_payment_frequency'] = text.replace('Ù…ÙˆØ§Ø¹Ø¯ Ù¾Ø±Ø¯Ø§Ø®Øª Ø³ÙˆØ¯ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ :', '').strip()
                elif 'Ø¨Ø¯ÙˆÙ† ØªØ¶Ù…ÛŒÙ† Ø³ÙˆØ¯' in text:
                    d['capital_guarantee'] = text
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒâ€ŒÙØ§Ù†Ø¯: {e}")
        return d

    def _scrape_zeema(self, project: IFBProject) -> Dict:
        """Ø²ÛŒÙ…Ù‡ â€“ Ø³Ø§Ø®ØªØ§Ø± Material-UI"""
        details = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            cards = soup.find_all('div', class_=lambda c: c and 'MuiGrid-root' in c)
            target = project.project_name.strip()
            for card in cards:
                # Ø¹Ù†ÙˆØ§Ù† Ø¯Ø± <span class="MuiTypography-root MuiTypography-subtitleBold">
                title_span = card.find('span', class_='MuiTypography-subtitleBold')
                if title_span and target in title_span.text:
                    details = self._extract_zeema_card(card)
                    break
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø²ÛŒÙ…Ù‡: {e}")
        return details

    def _extract_zeema_card(self, card) -> Dict:
        d = {}
        try:
            # Ø¹Ù†ÙˆØ§Ù†
            title = card.find('span', class_='MuiTypography-subtitleBold')
            if title:
                d['title_on_platform'] = title.text.strip()
            # ØªØµÙˆÛŒØ±
            img = card.find('img')
            if img and img.get('src'):
                d['thumbnail_url'] = img['src']
            # Ø´Ø±Ú©Øª/Ù…ØªÙ‚Ø§Ø¶ÛŒ
            company = card.find('span', class_='MuiTypography-smallMedium')
            if company:
                d['applicant_name'] = company.text.strip()
            # Ø³Ø±Ù…Ø§ÛŒÙ‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯
            req_divs = card.find_all('div', class_='MuiStack-root muirtl-bu0fgp')
            for div in req_divs:
                spans = div.find_all('span')
                if len(spans) >= 2:
                    label = spans[0].text.strip()
                    value = spans[1].text.strip()
                    if 'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²' in label:
                        d['target_amount'] = value
                    elif 'Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø³ÙˆØ¯ Ù¾Ø±ÙˆÚ˜Ù‡' in label:
                        d['expected_return'] = value
            # Ù…Ø¯Øª Ø·Ø±Ø­
            duration = card.find('div', class_='MuiStack-root muirtl-bl0m4')
            if duration:
                spans = duration.find_all('span')
                if len(spans) >= 2:
                    d['project_duration'] = spans[1].text.strip()
            # Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ
            fin = card.find('div', class_='MuiStack-root muirtl-bl0m4', string=re.compile('Ù†Ø§Ù… Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ'))
            if fin:
                spans = fin.find_all('span')
                if len(spans) >= 2:
                    d['financial_institution'] = spans[1].text.strip()
            # ØªØ¶Ù…ÛŒÙ†
            guar = card.find('div', class_='MuiStack-root muirtl-14mq6mq')
            if guar:
                d['capital_guarantee'] = guar.text.strip()
            # Ø³Ø±Ù…Ø§ÛŒÙ‡ ØªØ§Ù…ÛŒÙ† Ø´Ø¯Ù‡
            collected = card.find('div', class_='MuiStack-root muirtl-1pbtxwi')
            if collected:
                spans = collected.find_all('span')
                if len(spans) >= 2:
                    d['collected_amount'] = spans[1].text.strip()
            # Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´Ø±ÙØª
            progress = card.find('div', class_='MuiLinearProgress-root')
            if progress and progress.has_attr('aria-valuenow'):
                d['progress_percentage'] = progress['aria-valuenow']
            # ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±Ø§Ù†
            investors = card.find('div', class_='MuiStack-root muirtl-mk4amx')
            if investors:
                spans = investors.find_all('span')
                if len(spans) >= 2:
                    d['investor_count'] = spans[1].text.strip()
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²ÛŒÙ…Ù‡: {e}")
        return d

    # ---------- Ù…ØªØ¯ Ø¹Ù…ÙˆÙ…ÛŒ (fallback) ----------
    def _scrape_generic(self, project: IFBProject) -> Dict:
        """ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ (Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø¨Ù„ØºØŒ Ø³ÙˆØ¯ØŒ ...)"""
        d = {}
        try:
            self.driver.get(project.platform_url)
            time.sleep(self.config['delay'])
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            page_text = soup.get_text()
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
            patterns = {
                'target_amount': [r'Ù…Ø¨Ù„Øº Ù‡Ø¯Ù.*?([\d,Ù¬]+)', r'Ù‡Ø¯Ù.*?([\d,Ù¬]+)\s*ØªÙˆÙ…Ø§Ù†', r'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø².*?([\d,Ù¬]+)'],
                'expected_return': [r'(\d+\.?\d*)\s*Ùª', r'Ø³ÙˆØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ.*?(\d+\.?\d*)', r'Ø¨Ø§Ø²Ø¯Ù‡.*?(\d+\.?\d*)'],
                'project_duration': [r'(\d+)\s*Ù…Ø§Ù‡', r'Ù…Ø¯Øª Ø·Ø±Ø­.*?(\d+)\s*Ù…Ø§Ù‡'],
                'investor_count': [r'(\d+)\s*Ù†ÙØ±', r'ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±.*?(\d+)'],
            }
            for field, pat_list in patterns.items():
                for pat in pat_list:
                    match = re.search(pat, page_text, re.IGNORECASE)
                    if match:
                        d[field] = match.group(1)
                        break
            # Ù†Ø§Ù… Ø´Ø±Ú©Øª/Ù…ØªÙ‚Ø§Ø¶ÛŒ
            company_patterns = [r'Ø´Ø±Ú©Øª\s*([\w\s]+)', r'Ù…ØªÙ‚Ø§Ø¶ÛŒ\s*:\s*([\w\s]+)']
            for pat in company_patterns:
                match = re.search(pat, page_text)
                if match:
                    d['applicant_name'] = match.group(1).strip()
                    break
            # Ù†Ø§Ù… Ø³Ú©Ùˆ (Ø§Ø² Ø¯Ø§Ù…Ù†Ù‡)
            d['platform_name'] = urlparse(project.platform_url).netloc
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…ØªØ¯ Ø¹Ù…ÙˆÙ…ÛŒ: {e}")
        return d

    def _scroll_page(self, times=2):
        for _ in range(times):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)


# -------------------- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Google Sheets (Ø§ÙØ²Ø§ÛŒØ´ÛŒ) --------------------
class GoogleSheetsHandler:
    def __init__(self, credentials_dict: dict = None, credentials_path: str = 'service_account.json'):
        self.credentials_dict = credentials_dict
        self.credentials_path = credentials_path
        self.client = self._authenticate()

    def _authenticate(self):
        try:
            scopes = ['https://www.googleapis.com/auth/spreadsheets',
                      'https://www.googleapis.com/auth/drive']
            if self.credentials_dict:
                credentials = Credentials.from_service_account_info(self.credentials_dict, scopes=scopes)
            else:
                if not os.path.exists(self.credentials_path):
                    logger.warning(f"ÙØ§ÛŒÙ„ {self.credentials_path} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    return None
                credentials = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)
            return gspread.authorize(credentials)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Google Sheets: {str(e)}")
            return None

    def get_existing_ids(self, sheet_name: str, worksheet_index: int = 0) -> set:
        if not self.client:
            return set()
        try:
            spreadsheet = self.client.open(sheet_name)
            worksheet = spreadsheet.get_worksheet(worksheet_index)
            if not worksheet:
                return set()
            headers = worksheet.row_values(1)
            try:
                col_index = headers.index('ifb_project_id') + 1
            except ValueError:
                logger.warning("Ø³ØªÙˆÙ† 'ifb_project_id' Ø¯Ø± Ø´ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                return set()
            ids = worksheet.col_values(col_index)[1:]
            return set(ids)
        except gspread.SpreadsheetNotFound:
            logger.info(f"Ø´ÛŒØª {sheet_name} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            return set()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {e}")
            return set()

    def append_new_rows(self, sheet_name: str, data: List[Dict], id_field: str = 'ifb_project_id'):
        if not self.client:
            logger.error("Google Sheets client not available.")
            return False

        try:
            spreadsheet = self.client.open(sheet_name)
            worksheet = spreadsheet.sheet1
        except gspread.SpreadsheetNotFound:
            logger.info(f"Ø´ÛŒØª {sheet_name} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯...")
            spreadsheet = self.client.create(sheet_name)
            worksheet = spreadsheet.sheet1
            if data:
                headers = list(data[0].keys())
                worksheet.append_row(headers)
                logger.info("Ù‡Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯.")

        existing_ids = self.get_existing_ids(sheet_name)
        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø´ÛŒØª: {len(existing_ids)}")

        new_rows = []
        for item in data:
            pid = str(item.get(id_field, ''))
            if pid and pid not in existing_ids:
                new_rows.append(list(item.values()))
            elif not pid:
                logger.warning(f"Ø±Ø¯ÛŒÙ Ø¨Ø¯ÙˆÙ† Ø´Ù†Ø§Ø³Ù‡: {item.get('project_name', '')} - Ø§Ø¶Ø§ÙÙ‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

        if new_rows:
            worksheet.append_rows(new_rows)
            logger.info(f"ØªØ¹Ø¯Ø§Ø¯ {len(new_rows)} Ø±Ø¯ÛŒÙ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø´ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
        else:
            logger.info("Ù‡ÛŒÚ† Ø±Ø¯ÛŒÙ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

        logger.info(f"Ù„ÛŒÙ†Ú© Ø´ÛŒØª: https://docs.google.com/spreadsheets/d/{spreadsheet.id}")
        return True


# -------------------- ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ --------------------
def main():
    logger.info("=" * 60)
    logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø·Ø±Ø­â€ŒÙ‡Ø§ÛŒ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ Ø¬Ù…Ø¹ÛŒ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†")
    logger.info(f"Ø¢Ø¯Ø±Ø³: {IFB_MAIN_URL}")
    logger.info("=" * 60)

    # Ø®ÙˆØ§Ù†Ø¯Ù† credentials Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ (Ø¨Ø±Ø§ÛŒ GitHub Actions)
    creds_json = os.environ.get(CREDS_ENV_VAR)
    sheets_handler = None
    if creds_json:
        try:
            creds_dict = json.loads(creds_json)
            sheets_handler = GoogleSheetsHandler(credentials_dict=creds_dict)
            logger.info("âœ… Ø§Ø¹ØªØ¨Ø§Ø±Ù†Ø§Ù…Ù‡ Google Sheets Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯.")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† GOOGLE_CREDENTIALS: {e}")
    else:
        # fallback Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù…Ø­Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ù…Ø­Ù„ÛŒ)
        sheets_handler = GoogleSheetsHandler(credentials_path='service_account.json')
        logger.info("ğŸ“ Ø§Ø² ÙØ§ÛŒÙ„ Ù…Ø­Ù„ÛŒ service_account.json Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    scraper = IFBScraper(headless=False)  # Ø¯Ø± GitHub Actions headless=True Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ…
    try:
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Û±Û´Û°Û´ Ø§Ø² ÙØ±Ø§Ø¨ÙˆØ±Ø³
        projects = scraper.scrape_all_pages()
        if not projects:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø§ ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Û±Û´Û°Û´ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return

        logger.info(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ {len(projects)} Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Û±Û´Û°Û´ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")

        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø² Ø³Ú©ÙˆÙ‡Ø§
        logger.info("\n" + "=" * 60)
        logger.info("Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø² Ø³Ú©ÙˆÙ‡Ø§")
        logger.info("=" * 60)

        # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø´ÛŒØ¡ PlatformDetailScraper Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø¨Ø§ Ù‡Ù…Ø§Ù† driver)
        detail_scraper = PlatformDetailScraper(scraper.driver, scraper.config)
        enriched_projects = []
        for idx, proj in enumerate(projects, 1):
            logger.info(f"\n[{idx}/{len(projects)}] Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±ÙˆÚ˜Ù‡: {proj.project_name}")
            details = detail_scraper.scrape(proj)
            # ØªØ±Ú©ÛŒØ¨ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            combined = proj.to_dict()
            combined.update(details)
            enriched_projects.append(combined)
            logger.info(f"   âœ… {len(details)} ÙÛŒÙ„Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
            time.sleep(scraper.config['delay'])

        # Ù…Ø±Ø­Ù„Ù‡ 3: Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­Ù„ÛŒ (JSON Ùˆ CSV)
        base_filename = "ifb_projects_1404_complete"
        with open(f"{base_filename}.json", 'w', encoding='utf-8') as f:
            json.dump(enriched_projects, f, ensure_ascii=False, indent=4)
        logger.info(f"ğŸ’¾ ÙØ§ÛŒÙ„ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {base_filename}.json")

        df = pd.DataFrame(enriched_projects)
        df.to_csv(f"{base_filename}.csv", index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ ÙØ§ÛŒÙ„ CSV Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {base_filename}.csv")

        # Ù…Ø±Ø­Ù„Ù‡ 4: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§ÙØ²Ø§ÛŒØ´ÛŒ Ø¨Ù‡ Google Sheets
        if sheets_handler and sheets_handler.client:
            sheets_handler.append_new_rows(SHEET_NAME, enriched_projects, id_field='ifb_project_id')
        else:
            logger.warning("âš ï¸ Google Sheets Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡
        logger.info("\nğŸ“Š Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Û³ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§ÙˆÙ„):")
        for i, item in enumerate(enriched_projects[:3]):
            logger.info(f"\nÙ¾Ø±ÙˆÚ˜Ù‡ {i+1}: {item.get('project_name')}")
            logger.info(f"   Ø´Ù†Ø§Ø³Ù‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³: {item.get('ifb_project_id', '---')}")
            logger.info(f"   Ù…Ø¨Ù„Øº Ù‡Ø¯Ù: {item.get('target_amount', '---')}")
            logger.info(f"   Ù…Ø¯Øª Ø·Ø±Ø­: {item.get('project_duration', '---')}")
            logger.info(f"   Ø³ÙˆØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: {item.get('expected_return', '---')}")
            logger.info(f"   Ù…ØªÙ‚Ø§Ø¶ÛŒ: {item.get('applicant_name', '---')}")

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        scraper.close()


if __name__ == "__main__":
    main()